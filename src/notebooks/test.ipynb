{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "901768ad-85a5-4c1e-8c86-e195ae39af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0abf07eb-1df4-40c3-ac55-42bb7ed7018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter √ºzerinden Spark baƒülantƒ±sƒ± ba≈üarƒ±lƒ±!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg_Jupyter_Access\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"rest\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"iceberg\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Jupyter √ºzerinden Spark baƒülantƒ±sƒ± ba≈üarƒ±lƒ±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d0e67d3f-33c5-45f3-9d9b-d646e809c52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|      iceberg|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW CATALOGS').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5a63df11-b15d-4241-a4b4-c941d3a6df96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE NAMESPACE iceberg.demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "472137ba-50b8-4c3b-b752-f4e5781269fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|     demo|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW NAMESPACES in iceberg').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "73396beb-01e3-40fa-a94b-c784c852e660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('USE iceberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4ab0986a-d651-4d93-bf26-95f49dab5df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE TABLE iceberg.demo.test (id integer,name STRING)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "06c9307c-6e64-4d14-8285-71fc4754ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".                \n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO iceberg.demo.test (id,name) values(1,'ramazan'),(2,'eren')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "757a5819-37f5-494e-b728-0f66fbd2dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|ramazan|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM iceberg.demo.test where name=\\'ramazan\\'').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a03679c2-0797-4bf9-aa79-e7414403d5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('ALTER TABLE iceberg.demo.test add partition field id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "108d4e0a-7ddf-4d68-b619-12f0367b3bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|                  id|      int|   NULL|\n",
      "|                name|   string|   NULL|\n",
      "|# Partition Infor...|         |       |\n",
      "|          # col_name|data_type|comment|\n",
      "|                  id|      int|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE TABLE iceberg.demo.test').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "15540e60-7f78-46eb-8105-eaf85394bf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO iceberg.demo.test (id,name) values(3,'ahmet'),(4,'mehmet')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6ebe9027-6878-4853-9f44-a8847bc9f719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint, failed_data_files_count: int]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CALL iceberg.system.rewrite_data_files(table => 'iceberg.demo.test', strategy => 'sort',sort_order => 'id')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7150c06b-3f32-4ac9-86f3-263ebf9fc212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iceberg\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.currentCatalog())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a027cc09-722a-4492-afa8-3ced39657877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('id = 1)\n",
      "   +- 'UnresolvedRelation [iceberg, demo, test], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string\n",
      "Project [id#709, name#710]\n",
      "+- Filter (id#709 = 1)\n",
      "   +- SubqueryAlias iceberg.demo.test\n",
      "      +- RelationV2[id#709, name#710] iceberg.demo.test iceberg.demo.test\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(id#709) AND (id#709 = 1))\n",
      "+- RelationV2[id#709, name#710] iceberg.demo.test\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(id#709) AND (id#709 = 1))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan iceberg.demo.test[id#709, name#710] iceberg.demo.test (branch=null) [filters=id IS NOT NULL, id = 1, groupedBy=] RuntimeFilters: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. √ñnce Spark'ƒ±n planƒ±na bakalƒ±m\n",
    "df = spark.sql(\"SELECT * FROM iceberg.demo.test WHERE id = 1\")\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "773ec82e-6375-402a-b08b-566ddc614646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+-------------------+\n",
      "|     made_current_at|       snapshot_id|         parent_id|is_current_ancestor|\n",
      "+--------------------+------------------+------------------+-------------------+\n",
      "|2026-01-17 19:18:...|196083004806790204|              NULL|               true|\n",
      "|2026-01-17 19:22:...|450108357846897560|196083004806790204|               true|\n",
      "+--------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg.demo.test.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "49a1bd81-69c7-4d64-bfb2-638d993052ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  4| mehmet|\n",
      "|  1|ramazan|\n",
      "|  3|  ahmet|\n",
      "|  2|   eren|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg.demo.test FOR SYSTEM_TIME AS OF 450108357846897560\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "92d868ef-f0ff-4ecb-999f-0bcee3a9350e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DELETE FROM iceberg.demo.test where name=\"ramazan\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "aad374c6-afed-4907-ba10-76e9a8472bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  2|  eren|\n",
      "|  4|mehmet|\n",
      "|  3| ahmet|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select *  from iceberg.demo.test').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ba553879-791c-4bf2-98e8-1b30d28312c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|         parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+------------------+-------------------+\n",
      "|2026-01-17 19:18:...| 196083004806790204|              NULL|               true|\n",
      "|2026-01-17 19:22:...| 450108357846897560|196083004806790204|               true|\n",
      "|2026-01-17 19:29:...|3473538479800572684|450108357846897560|               true|\n",
      "+--------------------+-------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg.demo.test.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bd13e31f-9b65-46be-b725-bb47bc645ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|  name|\n",
      "+---+------+\n",
      "|  4|mehmet|\n",
      "|  3| ahmet|\n",
      "|  2|  eren|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_eskiveri = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"as-of-time\", \"3473538479800572684\") \\\n",
    "    .load(\"iceberg.demo.test\")\n",
    "\n",
    "df_eskiveri.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "887f61ac-8214-4a2d-b01a-a3a4475da440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE iceberg.demo.test RENAME COLUMN id TO user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e8ebd2c6-ff89-410e-9291-17b1beefa67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE iceberg.demo.test ADD COLUMN status STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "39fadb25-f963-465b-ab51-3767ab0f7598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|user_id|  name|status|\n",
      "+-------+------+------+\n",
      "|      2|  eren|  NULL|\n",
      "|      4|mehmet|  NULL|\n",
      "|      3| ahmet|  NULL|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg.demo.test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d8db4394-115e-47a0-82d1-6d6d8811618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "08da3a6b-8713-4647-98cb-1aa8d31e8d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ --- Starting high_volume_for_hire_vehicle ---\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2009. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2010. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2011. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2012. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2013. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2014. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2015. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2016. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2017. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2018. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/17 19:49:41 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.18.0.6 executor 1): org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: INT>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"INT\" and \"STRING\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n",
      "26/01/17 19:49:41 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error processing high_volume_for_hire_vehicle/2019: An error occurred while calling o2783.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 4) (172.18.0.6 executor 1): org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: INT>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"INT\" and \"STRING\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: INT>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"INT\" and \"STRING\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/17 19:49:43 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 6) (172.18.0.4 executor 0): org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"INT\" and \"DOUBLE\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n",
      "26/01/17 19:49:43 ERROR TaskSetManager: Task 3 in stage 1.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error processing high_volume_for_hire_vehicle/2020: An error occurred while calling o2790.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 4 times, most recent failure: Lost task 3.3 in stage 1.0 (TID 14) (172.18.0.6 executor 1): org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"INT\" and \"DOUBLE\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: INT, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"INT\" and \"DOUBLE\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/17 19:49:45 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing high_volume_for_hire_vehicle - Year: 2021 (174596652 rows)...\n",
      "üÜï Creating new table: iceberg.nyc.high_volume_for_hire_vehicle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Year 2021 committed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing high_volume_for_hire_vehicle - Year: 2022 (212416083 rows)...\n",
      "‚ûï Appending to existing table: iceberg.nyc.high_volume_for_hire_vehicle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Year 2022 committed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/17 20:00:04 WARN TaskSetManager: Lost task 0.0 in stage 22.0 (TID 288) (172.18.0.4 executor 0): org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: INT, DOLocationID: INT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"BIGINT\" and \"INT\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n",
      "26/01/17 20:00:04 ERROR TaskSetManager: Task 0 in stage 22.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error processing high_volume_for_hire_vehicle/2023: An error occurred while calling o2834.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 4 times, most recent failure: Lost task 0.3 in stage 22.0 (TID 294) (172.18.0.4 executor 0): org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: INT, DOLocationID: INT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"BIGINT\" and \"INT\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:\n",
      "Initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: BIGINT, DOLocationID: BIGINT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\"\n",
      "Schema that cannot be merged with the initial schema:\n",
      "\"STRUCT<hvfhs_license_num: STRING, dispatching_base_num: STRING, originating_base_num: STRING, request_datetime: TIMESTAMP_NTZ, on_scene_datetime: TIMESTAMP_NTZ, pickup_datetime: TIMESTAMP_NTZ, dropoff_datetime: TIMESTAMP_NTZ, PULocationID: INT, DOLocationID: INT, trip_miles: DOUBLE, trip_time: BIGINT, base_passenger_fare: DOUBLE, tolls: DOUBLE, bcf: DOUBLE, sales_tax: DOUBLE, congestion_surcharge: DOUBLE, airport_fee: DOUBLE, tips: DOUBLE, driver_pay: DOUBLE, shared_request_flag: STRING, shared_match_flag: STRING, access_a_ride_flag: STRING, wav_request_flag: STRING, wav_match_flag: STRING>\".\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingSchemaError(QueryExecutionErrors.scala:2175)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4$adapted(SchemaMergeUtils.scala:86)\n",
      "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:86)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_MERGE_INCOMPATIBLE_DATA_TYPE] Failed to merge incompatible data types \"BIGINT\" and \"INT\". Please check the data types of the columns being merged and ensure that they are compatible. If necessary, consider casting the columns to compatible data types before attempting the merge.\n",
      "\tat org.apache.spark.sql.errors.DataTypeErrors$.cannotMergeIncompatibleDataTypesError(DataTypeErrors.scala:162)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$3(StructType.scala:588)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:580)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$2$adapted(StructType.scala:577)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:577)\n",
      "\tat org.apache.spark.sql.types.StructType$.mergeInternal(StructType.scala:625)\n",
      "\tat org.apache.spark.sql.types.StructType$.merge(StructType.scala:567)\n",
      "\tat org.apache.spark.sql.types.StructType.merge(StructType.scala:482)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$4(SchemaMergeUtils.scala:88)\n",
      "\t... 19 more\n",
      "\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2024. Skipping...\n",
      "‚ö†Ô∏è  No data found for high_volume_for_hire_vehicle in 2025. Skipping...\n",
      "üèÅ high_volume_for_hire_vehicle processing finished.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Iceberg Schema Evolution ayarlarƒ± ile Spark'ƒ± ba≈ülatƒ±yoruz\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi to Iceberg Optimized\") \\\n",
    "      .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"rest\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://iceberg-rest:8181\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://warehouse/\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"iceberg\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "taxi_types = ['high_volume_for_hire_vehicle']\n",
    "years = [str(y) for y in range(2009, 2026)]\n",
    "\n",
    "# Veritabanƒ±nƒ± olu≈ütur\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS iceberg.nyc\")\n",
    "\n",
    "for taxi_type in taxi_types:\n",
    "    print(f\"\\nüöÄ --- Starting {taxi_type} ---\")\n",
    "    \n",
    "    table_name = f\"iceberg.nyc.{taxi_type}\"\n",
    "    \n",
    "    for year in years:\n",
    "        # Not: NYC verisi bazen ay ay da klas√∂rlenmi≈ü olabilir, wildcard (*) hayat kurtarƒ±r.\n",
    "        raw_path = f\"s3a://nyc-taxi/Dataset/{year}/{taxi_type}/*.parquet\"\n",
    "        \n",
    "        try:\n",
    "            # 1. Veriyi Oku (mergeSchema opsiyonu ile par√ßa par√ßa okumalarda risk azalƒ±r)\n",
    "            # S3'te dosya yoksa hata vermemesi i√ßin try-except bloƒüu kritik.\n",
    "            try:\n",
    "                df = spark.read.option(\"mergeSchema\", \"true\").parquet(raw_path)\n",
    "            except AnalysisException:\n",
    "                print(f\"‚ö†Ô∏è  No data found for {taxi_type} in {year}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            if df.count() > 0:\n",
    "                print(f\"Processing {taxi_type} - Year: {year} ({df.count()} rows)...\")\n",
    "\n",
    "                # 2. S√ºtun ƒ∞simlerini Normalize Et (Hepsini k√º√ß√ºk harfe √ßevir)\n",
    "                # NYC verisinde VendorID ve vendor_id karma≈üasƒ± √ßoktur.\n",
    "                df = df.toDF(*[c.lower() for c in df.columns])\n",
    "\n",
    "                # Metadata s√ºtunlarƒ±nƒ± ekle\n",
    "                df = df.withColumn(\"taxi_type\", lit(taxi_type)) \\\n",
    "                       .withColumn(\"file_year\", lit(year))\n",
    "\n",
    "                # 3. Tablo Kontrol√º ve Yazma ƒ∞≈ülemi\n",
    "                if not spark.catalog.tableExists(table_name):\n",
    "                    print(f\"üÜï Creating new table: {table_name}\")\n",
    "                    \n",
    "                    # Tabloyu olu≈ütururken Schema Evolution'a izin veriyoruz\n",
    "                    df.writeTo(table_name) \\\n",
    "                        .tableProperty(\"write.format.default\", \"parquet\") \\\n",
    "                        .tableProperty(\"write.spark.accept-any-schema\", \"true\") \\\n",
    "                        .partitionedBy(\"file_year\") \\\n",
    "                        .create()  # createOrReplace kullanma, yanlƒ±≈ülƒ±kla silme riskin var!\n",
    "                else:\n",
    "                    print(f\"‚ûï Appending to existing table: {table_name}\")\n",
    "                    \n",
    "                    # Iceberg otomatik olarak ≈üemayƒ± geni≈ületecek (yeni s√ºtun gelirse ekler)\n",
    "                    df.writeTo(table_name) \\\n",
    "                        .option(\"merge-schema\", \"true\") \\\n",
    "                        .option(\"check-ordering\", \"false\") \\\n",
    "                        .append()\n",
    "\n",
    "                print(f\"‚úÖ Year {year} committed.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {taxi_type}/{year}: {str(e)}\")\n",
    "            # Hata olsa bile d√∂ng√ºy√º kƒ±rma, sonraki yƒ±la ge√ß\n",
    "            continue\n",
    "\n",
    "    print(f\"üèÅ {taxi_type} processing finished.\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c5514e16-7e6a-4da8-bd83-542c733d218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /opt/bitnami/python/lib/python3.11/site-packages/pip-23.3.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting ipython-sql\n",
      "  Downloading ipython_sql-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting prettytable (from ipython-sql)\n",
      "  Downloading prettytable-3.17.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: ipython in /opt/bitnami/python/lib/python3.11/site-packages (from ipython-sql) (9.9.0)\n",
      "Collecting sqlalchemy>=2.0 (from ipython-sql)\n",
      "  Downloading sqlalchemy-2.0.45-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting sqlparse (from ipython-sql)\n",
      "  Downloading sqlparse-0.5.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six in /opt/bitnami/python/lib/python3.11/site-packages (from ipython-sql) (1.17.0)\n",
      "Collecting ipython-genutils (from ipython-sql)\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata (755 bytes)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=2.0->ipython-sql)\n",
      "  Downloading greenlet-3.3.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/bitnami/python/lib/python3.11/site-packages (from sqlalchemy>=2.0->ipython-sql) (4.15.0)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/bitnami/python/lib/python3.11/site-packages (from ipython->ipython-sql) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /opt/bitnami/python/lib/python3.11/site-packages (from prettytable->ipython-sql) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/bitnami/python/lib/python3.11/site-packages (from jedi>=0.18.1->ipython->ipython-sql) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/bitnami/python/lib/python3.11/site-packages (from pexpect>4.3->ipython->ipython-sql) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/bitnami/python/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython->ipython-sql) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/bitnami/python/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython->ipython-sql) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in /opt/bitnami/python/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython->ipython-sql) (0.2.3)\n",
      "Downloading ipython_sql-0.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading sqlalchemy-2.0.45-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Downloading prettytable-3.17.0-py3-none-any.whl (34 kB)\n",
      "Downloading sqlparse-0.5.5-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m468.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.3.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (590 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m590.2/590.2 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ipython-genutils, sqlparse, prettytable, greenlet, sqlalchemy, ipython-sql\n",
      "\u001b[33m  WARNING: The script sqlformat is installed in '/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed greenlet-3.3.0 ipython-genutils-0.2.0 ipython-sql-0.5.0 prettytable-3.17.0 sqlalchemy-2.0.45 sqlparse-0.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "48bbf3ee-eb9e-49cf-87c8-fe7240bcbdfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sql'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[172]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mload_ext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msql\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2511\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2509\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2511\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2513\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2514\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2515\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.11/site-packages/IPython/core/magics/extension.py:33\u001b[39m, in \u001b[36mExtensionMagics.load_ext\u001b[39m\u001b[34m(self, module_str)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_str:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[33m'\u001b[39m\u001b[33mMissing module name.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextension_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res == \u001b[33m'\u001b[39m\u001b[33malready loaded\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m extension is already loaded. To reload it, use:\u001b[39m\u001b[33m\"\u001b[39m % module_str)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.11/site-packages/IPython/core/extensions.py:62\u001b[39m, in \u001b[36mExtensionManager.load_extension\u001b[39m\u001b[34m(self, module_str)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[33;03mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01min\u001b[39;00m BUILTINS_EXTS:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.11/site-packages/IPython/core/extensions.py:77\u001b[39m, in \u001b[36mExtensionManager._load_extension\u001b[39m\u001b[34m(self, module_str)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shell.builtin_trap:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys.modules:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m         mod = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     mod = sys.modules[module_str]\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_load_ipython_extension(mod):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/python/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1204\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1140\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sql'"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f69d5f60-71ef-4d04-9d5a-e1f53490fb25",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3040630173.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[170]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mSELECT vendorid, COUNT(*)\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%sql\n",
    "SELECT vendorid, COUNT(*) \n",
    "FROM iceberg.nyc.yellow_taxi \n",
    "GROUP BY 1 \n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7e89a-06f6-4ff7-b32e-67f459d021ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
